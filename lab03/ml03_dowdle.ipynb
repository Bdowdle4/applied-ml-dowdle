{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95589812",
   "metadata": {},
   "source": [
    "# Dowdle's Titanic Survival Prediction\n",
    "**Author:** Brittany Dowdle  \n",
    "**Date:** March 26, 2025  \n",
    "**Objective:** Use the data you inspected, explored, and cleaned previously. Use 3 models to predict survival on the Titanic from various input features. Compare model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa9e4b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project uses the Titanic dataset to predict survival based on features such as class, sex, and family size. We will train multiple models, evaluate performance using key metrics, and create visualizations to interpret the results. We use three common classification models in this lab: Decision Tree Classifier (DT), Support Vector Machine (SVM), and Neural Network (NN).\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "In the code cell below, import the necessary Python libraries for this notebook. All imports should be at the top of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data manipulation and analysis (we might want to do more with it)\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Import pandas for data manipulation and analysis  (we might want to do more with it)\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for creating static visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Import seaborn for statistical data visualization (built on matplotlib)\n",
    "import seaborn as sns\n",
    "\n",
    "# Import train_test_split for splitting data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Import classification models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import performance metrics for model evaluation\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Section 1. Import and Inspect the Data\n",
    "\n",
    "We don't need to inspect the data as we've already done that and are familiar with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## Section 2. Data Exploration and Preparation\n",
    "We might need to clean it or do some feature engineering. Learning to figure out what you need is a key skill.\n",
    "\n",
    "### 2.1 Handle Missing Values and Clean Data\n",
    "\n",
    "- Impute missing values for age using the median.\n",
    "- Fill in missing values for embark_town using the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for age using the median \n",
    "titanic.fillna({'age': titanic['age'].median()}, inplace=True)\n",
    "\n",
    "# Fill missing values for embark_town using the mode\n",
    "titanic['embark_town'] = titanic['embark_town'].fillna(titanic['embark_town'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering\n",
    "\n",
    "- Add family_size - number of family members on board.\n",
    "- Convert categorical \"sex\" to numeric.\n",
    "- Convert categorical \"embarked\" to numeric.\n",
    "- Binary feature - convert \"alone\" to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a869bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create family_size\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "\n",
    "# Convert categorical to numeric\n",
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Convert categorical to numeric\n",
    "titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Convert binary to numeric\n",
    "titanic['alone'] = titanic['alone'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0acdb8",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n",
    "\n",
    "### 3.1 Choose features and target\n",
    "Use three input cases like the example.\n",
    "\n",
    ">First:\n",
    ">\n",
    ">input features: alone\n",
    ">\n",
    ">target: survived\n",
    ">\n",
    ">Second:\n",
    ">\n",
    ">input features - age\n",
    ">\n",
    ">target: survived\n",
    ">\n",
    ">Third:\n",
    ">\n",
    ">input features -  age and family_size\n",
    ">\n",
    ">target: survived"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e11a66",
   "metadata": {},
   "source": [
    "### 3.2 Define X and y\n",
    "\n",
    "- Assign input features to X a pandas DataFrame with 1 or more input features\n",
    "- Assign target variable to y (as applicable) - a pandas Series with a single target feature\n",
    "- Use comments to run a single case at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "   alone\n",
      "0      0\n",
      "1      0\n",
      "2      1\n",
      "3      0\n",
      "4      1\n",
      "\n",
      "Target Variable:\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Pro tip: Double brackets [[ ]]] makes a 2D DataFrame. Single brackets [ ] make a 1D Series.\n",
    "# Case 1: alone only \n",
    "X = titanic[['alone']]\n",
    "y = titanic['survived']\n",
    "\n",
    "# Case 2: age only\n",
    "# X = titanic[['age']]\n",
    "# y = titanic['survived']\n",
    "\n",
    "# Case 3: age + family_size\n",
    "# X = titanic[['age', 'family_size']]\n",
    "# y = titanic['survived']\n",
    "\n",
    "# Print the first few rows to confirm feature selection\n",
    "print(\"Selected Features:\")\n",
    "print(X.head())\n",
    "\n",
    "print(\"\\nTarget Variable:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection 3:\n",
    "\n",
    "1) Why are these features selected? **Alone: Determines who a passenger was traveling with, which could impact their ability to receive help or access lifeboats. Age: Younger passengers, especially children, were prioritized for lifeboats, while older passengers may have faced mobility challenges. Family Size: Traveling with family could improve survival chances due to mutual assistance, but larger families might have struggled to evacuate together.**\n",
    "2) Are there any features that are likely to be highly predictive of survival? **Yes. Sex: Women had much higher survival rates, Pclass: First-class passengers had better access to lifeboats, Fare: Higher fares correlated with better survival chances, Embarked: Port of embarkation might reflect social/economic factors.**\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676c419",
   "metadata": {},
   "source": [
    "## Section 4. Train a Classification Model (Decision Tree)\n",
    "\n",
    "\n",
    "### 4.1 Split the Data\n",
    "Split the data into training and test sets. Use StratifiedShuffleSplit to ensure even class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "967ed19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 712\n",
      "Test size: 179\n"
     ]
    }
   ],
   "source": [
    "# Define how many splits, % of data for testing, and ensure reproducibility\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\n",
    "\n",
    "# Split data into a training set and a test set\n",
    "for train_indices, test_indices in splitter.split(X, y):\n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_test = y.iloc[test_indices]\n",
    "\n",
    "# Show set sizes\n",
    "print('Train size:', len(X_train))\n",
    "print('Test size:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create and Train Model (Decision Tree)\n",
    "Create and train a decision tree model with no random initializer argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Depth: 1\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Show depth for confirmation\n",
    "print(f\"Tree Depth: {tree_model.get_depth()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f529b",
   "metadata": {},
   "source": [
    "### 4.3 Predict and Evaluate Model Performance\n",
    "\n",
    "- Evaluate model performance on training data.\n",
    "- Evaluate model performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df3db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Decision Tree on training data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       439\n",
      "           1       0.50      0.51      0.51       273\n",
      "\n",
      "    accuracy                           0.62       712\n",
      "   macro avg       0.60      0.60      0.60       712\n",
      "weighted avg       0.62      0.62      0.62       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training data (X_train)\n",
    "y_pred = tree_model.predict(X_train)  \n",
    "print(\"Results for Decision Tree on training data:\")  \n",
    "print(classification_report(y_train, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf478f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Decision Tree on test data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.65      0.68       110\n",
      "           1       0.51      0.58      0.54        69\n",
      "\n",
      "    accuracy                           0.63       179\n",
      "   macro avg       0.61      0.62      0.61       179\n",
      "weighted avg       0.64      0.63      0.63       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test data (X_test)\n",
    "y_test_pred = tree_model.predict(X_test)\n",
    "print(\"Results for Decision Tree on test data:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6b2c2",
   "metadata": {},
   "source": [
    "### Reflection 4:\n",
    "\n",
    "1) Why might stratification improve model performance? **The dataset was imbalanced across class, and stratification ensures that both the training and test sets maintain the same class distribution as the original dataset. It helps the model learn from a more representative sample of the data, which leads to more reliable performance.**\n",
    "2) How close are the training and test distributions to the original dataset? **Stratified - the distributions for both the training and test sets are very similar to the original proportions. Basic Split - some deviation in the test set, where Class 0 is slightly overrepresented.**\n",
    "3) Which split method produced better class balance? **Stratified Split produced a better class balance because it preserved the original class proportions more accurately in both the training and test sets.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
